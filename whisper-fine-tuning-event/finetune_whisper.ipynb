{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a331a01-74a9-4f4b-a198-7e15965cd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f96ee-abd0-4070-bd67-08c24f08eda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e4a64-1483-49c6-871e-b4d0e930e619",
   "metadata": {},
   "source": [
    "## Prepare Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a818-c6ad-488e-910c-ff32dcfa6356",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c3d35-191b-4900-a8b3-f4a4002133dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f78ae6-32fc-444f-9ca6-bb91caafac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include forced token in the training\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "# to use gradient checkpointing\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720113a-ecf3-40b3-bb46-e662517cfaf1",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a188c741-6330-489e-ae4f-c26b20b78788",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_COLUMN_NAME = \"audio\"\n",
    "TEXT_COLUMN_NAME = \"sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe80a2b-e464-4555-a18b-b69e0f09a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio, DatasetDict, concatenate_datasets, load_dataset\n",
    "\n",
    "def normalize_dataset(ds, audio_column_name=None, text_column_name=None):\n",
    "    if audio_column_name is not None and audio_column_name != AUDIO_COLUMN_NAME:\n",
    "        ds = ds.rename_column(audio_column_name, AUDIO_COLUMN_NAME)\n",
    "    if text_column_name is not None and text_column_name != TEXT_COLUMN_NAME:\n",
    "        ds = ds.rename_column(text_column_name, TEXT_COLUMN_NAME)\n",
    "    # resample to the same sampling rate\n",
    "    ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    # normalise columns to [\"audio\", \"sentence\"]\n",
    "    ds = ds.remove_columns(set(ds.features.keys()) - set([AUDIO_COLUMN_NAME, TEXT_COLUMN_NAME]))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2947e75c-6b6b-43e6-9025-e73b08cfde1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhuang/anaconda3/envs/jpt/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset common_voice_11_0 (/home/bhuang/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/fr/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f)\n",
      "Reusing dataset multilingual_librispeech (/home/bhuang/.cache/huggingface/datasets/facebook___multilingual_librispeech/french/2.1.0/1904af50f57a5c370c9364cc337699cfe496d4e9edcae6648a96be23086362d0)\n",
      "/home/bhuang/anaconda3/envs/jpt/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Reusing dataset common_voice_11_0 (/home/bhuang/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/fr/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 761752\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 16089\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "\n",
    "ds_train_mcv = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"train+validation\", use_auth_token=True)\n",
    "ds_train_mcv = normalize_dataset(ds_train_mcv)\n",
    "\n",
    "ds_train_mls = load_dataset(\"facebook/multilingual_librispeech\", \"french\", split=\"train+validation\")\n",
    "ds_train_mls = normalize_dataset(ds_train_mls, text_column_name=\"text\")\n",
    "\n",
    "raw_datasets[\"train\"] = concatenate_datasets([ds_train_mcv, ds_train_mls])\n",
    "\n",
    "# NB: shuffle concatenated dataset\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].shuffle(seed=10)\n",
    "\n",
    "raw_datasets[\"eval\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fr\", split=\"test\", use_auth_token=True)\n",
    "raw_datasets[\"eval\"] = normalize_dataset(raw_datasets[\"eval\"])\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0b2a6e-07ec-466f-b07a-5334e676da38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only use part of training and test set for demonstration\n",
    "# comment this\n",
    "num_train_examples = 500\n",
    "num_test_examples = 100\n",
    "\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(num_train_examples))\n",
    "raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(num_test_examples))\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67cab0-4394-4d27-924e-bbf4c517977e",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96471d8-ba77-4e88-8088-35c75e7a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download musan dataset for background noise\n",
    "# ! wget https://www.openslr.org/resources/17/musan.tar.gz\n",
    "# ! tar -zxvf musan.tar.gz\n",
    "# ! rm -r ./musan/speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed5bb9f-e3ba-4499-be32-6f5f2ab167a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import (\n",
    "    AddBackgroundNoise,\n",
    "    AddGaussianNoise,\n",
    "    Compose,\n",
    "    Gain,\n",
    "    OneOf,\n",
    "    PitchShift,\n",
    "    PolarityInversion,\n",
    "    TimeStretch,\n",
    ")\n",
    "\n",
    "musan_dir = \"./musan\"\n",
    "\n",
    "# define augmentation\n",
    "augmentation = Compose(\n",
    "    [\n",
    "        TimeStretch(min_rate=0.9, max_rate=1.1, p=0.2, leave_length_unchanged=False),\n",
    "        Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.1),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
    "        OneOf(\n",
    "            [\n",
    "                AddBackgroundNoise(\n",
    "                    sounds_path=musan_dir, min_snr_in_db=1.0, max_snr_in_db=5.0, noise_transform=PolarityInversion(), p=1.0\n",
    "                ),\n",
    "                AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1.0),\n",
    "            ],\n",
    "            p=0.2,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def augment_dataset(batch):\n",
    "    # load and (possibly) resample audio data to 16kHz\n",
    "    sample = batch[AUDIO_COLUMN_NAME]\n",
    "\n",
    "    # apply augmentation\n",
    "    augmented_waveform = augmentation(sample[\"array\"], sample_rate=sample[\"sampling_rate\"])\n",
    "    batch[AUDIO_COLUMN_NAME][\"array\"] = augmented_waveform\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0821a184-41e3-4176-99d9-e2b9df8b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18948b1a-1407-4634-bfc2-d45f22dcbd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function augment_dataset at 0x7f6083ea0f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33767306a3e94347a055234f18650794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augment train dataset #0:   0%|          | 0/125 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945aa4ec41a5442cb6a91782389b5df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augment train dataset #1:   0%|          | 0/125 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9162532212ba46c68bec099044dad3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augment train dataset #2:   0%|          | 0/125 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630fec62fe0e4eccbbd31ddfdabc6c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augment train dataset #3:   0%|          | 0/125 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augment training data\n",
    "augmented_raw_training_dataset = raw_datasets[\"train\"].map(\n",
    "    augment_dataset, num_proc=preprocessing_num_workers, desc=\"augment train dataset\"\n",
    ")\n",
    "\n",
    "# combine\n",
    "raw_datasets[\"train\"] = concatenate_datasets([raw_datasets[\"train\"], augmented_raw_training_dataset])\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].shuffle(seed=10)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6531c-eca0-4cd4-8d03-f89736023ee5",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e832df-5454-4cd0-8331-2f47870df9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7008338-9d03-4105-b7b0-bb7e98e5a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"french\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f4e685-100d-4b17-9196-9b2e17f477d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_normalize_text = True\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load\n",
    "    audio = batch[AUDIO_COLUMN_NAME]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[\n",
    "        0\n",
    "    ]\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    # process targets\n",
    "    input_str = normalizer(batch[TEXT_COLUMN_NAME]).strip() if do_normalize_text else batch[TEXT_COLUMN_NAME]\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(input_str).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da97e2-af05-4184-a7fc-b549c874a8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d970e247be4ae3b28b5a1de470ddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocess dataset:   0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_datasets = raw_datasets.map(\n",
    "    prepare_dataset, remove_columns=next(iter(raw_datasets.values())).column_names, desc=\"preprocess dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b382e-d98b-460a-9136-394977a37c06",
   "metadata": {},
   "source": [
    "### Remove Long Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15388390-39cc-4e7c-ad02-8ea7f1c521d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30\n",
    "min_input_length = 0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb7fe2-07df-4f21-80ba-690e7722c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range, num_proc=preprocessing_num_workers, input_columns=[\"input_length\"]\n",
    ")\n",
    "\n",
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3452dc-d7bf-4b02-8987-068b2f23c2b0",
   "metadata": {},
   "source": [
    "### Remove LongÂ Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4553f-cfe0-4a56-89a4-d4f7a570a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_length = model.config.max_length\n",
    "max_label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b6ca4-b2d8-4f32-ac59-ac8346df5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_labels_in_length_range(labels):\n",
    "    return len(labels) < max_label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75355b9-7230-4849-a86a-7251c558c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_labels_in_length_range, num_proc=preprocessing_num_workers, input_columns=[\"labels\"]\n",
    ")\n",
    "\n",
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb9469-c31c-4a6b-b28f-ea55c1d8551b",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1f413-cce6-4125-a950-cc91b52ba26d",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad66769-9094-4bd9-9aa9-5b7a06d9a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        # convert to tensors\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad label ids to the max length in the batch\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fd7e2-9ba0-41ad-ae03-183eca100a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e782e-5820-4233-841e-0b800acf5ee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4789691-1712-4c35-9c5f-88d3c150cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3669d1-e5d2-493e-b072-0ed184f541ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if do_normalize_eval:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        # perhaps already normalised\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "        # filtering step to only evaluate the samples that correspond to non-zero references\n",
    "        pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "        label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "\n",
    "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4635b22-2fff-4340-b20e-98151e9d9a19",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ca560-0408-419a-8bdb-e88afb241035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./outputs/whisper_medium_ft\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=800,\n",
    "    max_steps=8000,\n",
    "    learning_rate=6.25e-6,\n",
    "    weight_decay=0.01,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d01c5-9d1b-4524-80cc-5f9c87b6b2de",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddfa78-47d5-4bfe-b471-6233ec69313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"eval\"],\n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7e022-624c-4e1f-a527-428c58056d68",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19856d8-acdd-473a-b16c-7b6dae54cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a724efc-4528-41ca-8817-c2a757f7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df543330-5031-48b0-baad-ddff072ecaab",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5955f-7901-4e34-a758-9736083c065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
    "\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d011e9-45cf-4fd2-8742-1545e8dedd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
